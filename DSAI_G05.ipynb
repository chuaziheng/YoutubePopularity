{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CZ1016 Introduction to Data Science\n",
    "## Group Project\n",
    "### YouTube Statistics - What makes a popular YouTube video?\n",
    "\n",
    "\n",
    "#### Members:\n",
    "- Chua Zi Heng U1922370K\n",
    "- Mun Kei Wuai U1921982B\n",
    "- Tan Wen Xiu  U1921771H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim to find out which features are important in the classification of a popular YouTube video. We define popular as having a high (likes-dislikes)/views ratio. \n",
    "\n",
    "We will be using datasets from **United States of America** and **Great Britain**. The following project will be split into 2 parts - separate data cleaning and classifications will be carried out for each country. Thereafter, we will compare the results derived from both datasets and perform in-depth analysis.\n",
    "\n",
    "We have came up with 8 features that we think might be useful in the classification.\n",
    "\n",
    "Features (for each unique video): \n",
    "1. Number of views\n",
    "2. Number of likes\n",
    "3. Number of dislikes\n",
    "4. Number of trending days\n",
    "5. Number of comments\n",
    "6. Average Sentiment Score\n",
    "7. Number of positive comments \n",
    "8. Number of negative comments \n",
    "\n",
    "We have also built a univariate decision tree for each feature, a multivariate decsion tree and a random forest to find out which form of classification will give the highest classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_np_version_under1p18' from 'pandas.compat.numpy' (C:\\Users\\chuaz\\Anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-66e1a9ab629f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Basic Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0m_np_version_under1p14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0m_np_version_under1p15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_np_version_under1p18' from 'pandas.compat.numpy' (C:\\Users\\chuaz\\Anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set() # set the default Seaborn style for graphics\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import plotly.offline as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import tools\n",
    "# Activate inline plotting in notebook\n",
    "py.init_notebook_mode(connected = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. United States of America (US)\n",
    "- Since the video statistics and comments statistics are in separate CSV files, we will import both files (USvideos & UScomments) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import dataset and remove bad lines\n",
    "USvideos = pd.read_csv('USvideos.csv', error_bad_lines=False)\n",
    "USvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset and remove bad lines\n",
    "UScomments = pd.read_csv('UScomments.csv', error_bad_lines=False)\n",
    "UScomments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#del thumbnail column as it is redundant for classification\n",
    "USvideos = USvideos.drop(columns='thumbnail_link')\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort videos from most views to least views for visualisation \n",
    "USvideos.sort_values(by=['views'], inplace=True, ascending=False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from above that there are duplicates in the videos. For example, BTS (방탄소년단) 'DNA' Official MV appeared more than 1 time. Duplicates happen when the video is trending for more than 1 day. Thus, with this inference, we can use this data to derive the number of days a video has been trending for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'USvideos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e6d69a36a344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mUSvideos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'USvideos' is not defined"
     ]
    }
   ],
   "source": [
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new DataFrame for number of duplicates to show the number of duplicates\n",
    "duplicates = USvideos['video_id'].value_counts().rename_axis('video_id').reset_index(name='trending days')\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "USvideos.drop_duplicates(subset =\"video_id\", keep = 'first', inplace = True)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show that there are some videos with 0 views\n",
    "print(len(USvideos[USvideos['views'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show the percentage of videos with 0 views is very small so its is okay to remove\n",
    "print((len(USvideos[USvideos['views']==0])) / len(USvideos*100), '%', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with 0 views\n",
    "indexNames = USvideos[USvideos['views'] == 0].index\n",
    "USvideos.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merging 2 dataframes together, adding number of trending days\n",
    "USvideos = pd.merge(USvideos, duplicates, on = 'video_id')\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# take a look at dataset again: 2634 videos change to 2632 videos after removal of 2 videos with 0 views\n",
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be seen that there are some null objects under comment_text\n",
    "UScomments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UScomments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at rows which comments are null\n",
    "print(UScomments[UScomments['comment_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with null comments\n",
    "len(UScomments[UScomments['comment_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the row index which comment_text are null\n",
    "indexNames = UScomments[UScomments['comment_text'].isnull()].index\n",
    "indexNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with null comments\n",
    "UScomments.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the 25 rows are dropped\n",
    "UScomments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of rows in USvideos whose video_id are NOT in UScomments\n",
    "len(USvideos[~USvideos.video_id.isin(UScomments.video_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row index of rows in USvideos whose video_id are NOT in UScomments\n",
    "rowIndex1 = USvideos[~USvideos.video_id.isin(UScomments.video_id)].index\n",
    "print(rowIndex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows in UScomments whose video_id are NOT in USvideos\n",
    "USvideos.drop(rowIndex1, inplace=True)\n",
    "USvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at dataset again after 97 videos were dropped\n",
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at UScomments dataset\n",
    "UScomments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of rows in UScomments whose video_id are NOT in USvideos\n",
    "len(UScomments[~UScomments.video_id.isin(USvideos.video_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row index of rows in UScomments whose video_id are NOT in USvideos\n",
    "rowIndex2 = UScomments[~UScomments.video_id.isin(USvideos.video_id)].index\n",
    "print(rowIndex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows in UScomments whose video_id are NOT in USvideos\n",
    "UScomments.drop(rowIndex2, inplace=True)\n",
    "UScomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at dataset again after 888 videos were dropped\n",
    "UScomments.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman Rank Correlation\n",
    "- uses ranking between likes, views and popularity, popularity2 \n",
    "- choose a y-variable that is appropriate (ie. is normalised, which means a low spearman correlation coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of views \n",
    "USvideos['views_rank'] = USvideos['views'].rank(ascending = False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of likes\n",
    "USvideos['likes_rank'] = USvideos['views'].rank(ascending = False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set y as likes/views\n",
    "#sort based on likes over views \n",
    "USvideos['popularity'] = USvideos.apply(lambda row: 1000*(row.likes/row.views), axis = 1)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set y as (likes-dislikes)/views\n",
    "#sort based on (likes-dislikes)/views\n",
    "USvideos['popularity2'] = USvideos.apply(lambda row: 1000*(row.likes-row.dislikes)/row.views, axis = 1)\n",
    "\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of popularity \n",
    "USvideos['popularity_rank'] = USvideos['popularity'].rank(ascending = False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'USvideos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a9f3de42db34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create column for ranking of popularity2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'popularity2_rank'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'popularity2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mUSvideos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'USvideos' is not defined"
     ]
    }
   ],
   "source": [
    "#create column for ranking of popularity2\n",
    "USvideos['popularity2_rank'] = USvideos['popularity2'].rank(ascending = False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(USvideos[\"likes_rank\"], USvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the ranking of videos with the most number of views would correspond with that of the most number of likes. The rankings of these 2 features are essentially equivalent. Hence, the spearman correlation coefficient of \"views\" or \"likes\", with other features would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(USvideos[\"popularity_rank\"], USvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**popularity = likes/views**. This suggests a higher correlation between the ranks of popularity and views, rather than popularity2 and views as seen below, possibily because we did not take into account the number of dislikes of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(USvideos[\"popularity2_rank\"], USvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**popularity2 = (likes-dislikes)/views**. Lower correlation shows that popularity2 is not highly associated to the number of views (or likes) that the video receives. Hence, the response is considered to be normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort videos from most views to least views\n",
    "USvideos.sort_values(by=['views'], inplace=True, ascending=False)\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is sorted by highest to lowest views. Looking at popularity2_rank, it is noted that the highest views does not mean highest popularity2_rank. Hence, we will make use of popularity2 as the response variable for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_pop = go.Box(x = USvideos['popularity2'], showlegend = False, name = \"popularity2\")\n",
    "py.iplot([US_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at overall dataset\n",
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 point is very far away from the dataset, we will choose to separate them from the classification\n",
    "anomaly1 = USvideos[USvideos['popularity2']==USvideos['popularity2'].min()]\n",
    "USvideos = USvideos.loc[USvideos['popularity2']!=USvideos['popularity2'].min()]\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at overall dataset\n",
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for popularity2 after removing 1 anomaly\n",
    "US_pop = go.Box(x = USvideos['popularity2'], showlegend = False, name = \"popularity2\")\n",
    "py.iplot([US_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#categorize the popularity2 of the videos into 4 categories according to their quartiles \n",
    "USvideos['quantile_popularity'] = pd.cut(USvideos['popularity2'], bins = 4, labels = ['low', 'very low', 'very high','high'])\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USvideos['quantile_popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USvideos['popularity2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(USvideos[[\"views\", \"likes\", \"dislikes\", \"comment_total\", \"trending days\"]]) \n",
    "# Plot the Raw Data on 2D grids\n",
    "sb.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the distribution of Response\n",
    "f, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "sb.boxplot(USvideos['popularity2'], orient = \"h\", ax = axes[0], color = \"g\")\n",
    "sb.distplot(USvideos['popularity2'], kde = False, ax = axes[1], color = \"g\")\n",
    "sb.violinplot(USvideos['popularity2'], ax = axes[2], color = \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot for popularity2\n",
    "trace = go.Histogram(x = USvideos['popularity2'], histnorm = 'density')\n",
    "layout = go.Layout(title = 'Popularity2 Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot for quantile_popularity\n",
    "trace = go.Histogram(x = USvideos['quantile_popularity'], histnorm = 'density')\n",
    "layout = go.Layout(title = 'Quantile Popularity Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows that there is an uneven distribution of videos in each of the categories, with 'very low' having the most number of videos and 'high' having the least. This might be a limitation as those in 'very low' can be better trained. We will analyse more of this in the Analysis part later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for Further Data Visualisation\n",
    "### Bi-variate clustering by Kmeans++ \n",
    "- views & popularity2 are used because views are seen as the most straightforward way of predicting popularity in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(USvideos[[\"views\", \"popularity2\"]])\n",
    "\n",
    "# Set the Initialization to KMeans++\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Vary the Number of Clusters\n",
    "min_clust = 1\n",
    "max_clust = 40\n",
    "\n",
    "# Compute Within Cluster Sum of Squares\n",
    "within_ss = []\n",
    "for num_clust in range(min_clust, max_clust+1):\n",
    "    kmeans = KMeans(n_clusters = num_clust,        # number of clusters\n",
    "                    init = init_algo,              # initialization algorithm\n",
    "                    n_init = 5)                    # number of initializations\n",
    "    kmeans.fit(X)\n",
    "    within_ss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Within SS vs Number of Clusters\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,4))\n",
    "plt.plot(range(min_clust, max_clust+1), within_ss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within Cluster Sum of Squares')\n",
    "plt.xticks(np.arange(min_clust, max_clust+1, 1.0))\n",
    "plt.grid(which='major', axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the elbow plot, it can be seen that the optimal number of clusters is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"optimal\" Number of Clusters\n",
    "num_clust = 4\n",
    "\n",
    "# Set the Initialization to KMeans++\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Create Clustering Model using KMeans\n",
    "kmeans = KMeans(n_clusters = num_clust, init = init_algo, n_init = 20)                 \n",
    "\n",
    "# Fit the Clustering Model on the Data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print the Cluster Centers\n",
    "print(\"Features\", \"\\tviews\", \"\\tpopularity2\")\n",
    "print()\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    print(\"Cluster\", i, end=\":\\t\")\n",
    "    for coord in center:\n",
    "        print(round(coord, 2), end=\"\\t\")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "# Print the Within Cluster Sum of Squares\n",
    "print(\"Within Cluster Sum of Squares :\", kmeans.inertia_)\n",
    "print()\n",
    "\n",
    "# Predict the Cluster Labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Append Labels to the Data\n",
    "X_labeled = X.copy()\n",
    "X_labeled[\"Cluster\"] = pd.Categorical(labels)\n",
    "\n",
    "# Summary of the Cluster Labels\n",
    "sb.countplot(X_labeled[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Clusters in the Data\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "plt.scatter(x = \"views\", y = \"popularity2\", c = \"Cluster\", cmap = 'viridis', data = X_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the Features against the Clusters\n",
    "f, axes = plt.subplots(2, 1, figsize=(16,8))\n",
    "sb.boxplot(x = 'views', y = 'Cluster', data = X_labeled, ax = axes[0])\n",
    "sb.boxplot(x = 'popularity2', y = 'Cluster', data = X_labeled, ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate Clustering by Kmeans++\n",
    "- we decided that bi-variate clustering was not enough, thus we decided to use this method to get a better overview of the dataset\n",
    "- categorical data (ie. Category ID) and non-numerical data (ie comment_text) are not used in clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(USvideos[[\"likes\", \"views\", \"comment_total\", \"dislikes\"]]) \n",
    "\n",
    "\n",
    "# Plot the Raw Data on 2D grids\n",
    "sb.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the Number of Clusters\n",
    "min_clust = 1\n",
    "max_clust = 40\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Compute Within Cluster Sum of Squares\n",
    "within_ss = []\n",
    "for num_clust in range(min_clust, max_clust+1):\n",
    "    kmeans = KMeans(n_clusters = num_clust, init = init_algo, n_init = 5)\n",
    "    kmeans.fit(X)\n",
    "    within_ss.append(kmeans.inertia_)\n",
    "\n",
    "# Angle Plot : Within SS vs Number of Clusters\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,4))\n",
    "plt.plot(range(min_clust, max_clust+1), within_ss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within Cluster Sum of Squares')\n",
    "plt.xticks(np.arange(min_clust, max_clust+1, 1.0))\n",
    "plt.grid(which='major', axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set \"optimal\" Clustering Parameters\n",
    "num_clust = 4\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Create Clustering Model using KMeans\n",
    "kmeans = KMeans(n_clusters = num_clust,         \n",
    "               init = init_algo,\n",
    "               n_init = 20)                 \n",
    "\n",
    "# Fit the Clustering Model on the Data\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Cluster Centers\n",
    "print(\"Features\", \"\\tlikes\", \"\\tviews\", \"\\tcomment_total\", \"\\tdislikes\",)\n",
    "print()\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    print(\"Cluster\", i, end=\":\\t\")\n",
    "    for coord in center:\n",
    "        print(round(coord, 2), end=\"\\t\")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "# Print the Within Cluster Sum of Squares\n",
    "print(\"Within Cluster Sum of Squares :\", kmeans.inertia_)\n",
    "print()\n",
    "\n",
    "# Predict the Cluster Labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Append Labels to the Data\n",
    "X_labeled = X.copy()\n",
    "X_labeled[\"Cluster\"] = pd.Categorical(labels)\n",
    "\n",
    "# Summary of the Cluster Labels\n",
    "sb.countplot(X_labeled[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Clusters on 2D grids\n",
    "sb.pairplot(X_labeled, vars = X.columns.values, hue = \"Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for all Features against the Clusters\n",
    "f, axes = plt.subplots(4, 1, figsize=(16,24))\n",
    "sb.boxplot(x = 'likes', y = 'Cluster', data = X_labeled, ax = axes[0])\n",
    "sb.boxplot(x = 'views', y = 'Cluster', data = X_labeled, ax = axes[1])\n",
    "sb.boxplot(x = 'comment_total', y = 'Cluster', data = X_labeled, ax = axes[2])\n",
    "sb.boxplot(x = 'dislikes', y = 'Cluster', data = X_labeled, ax = axes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Average Behaviour of each Cluster\n",
    "cluster_data = pd.DataFrame(X_labeled.groupby(by = \"Cluster\").mean())\n",
    "cluster_data.plot.bar(figsize = (16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be moving on to the 8 features mentioned at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1: Number of Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-26d7b809408b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Recall the Legendary-Total Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mQP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'quantile_popularity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'views'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# Predictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Split the Legendary-Total Dataset into Train and Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "views = pd.DataFrame(USvideos['views'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(views, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 2: Number of Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "likes = pd.DataFrame(USvideos['likes'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(likes, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3: Number of Dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "dislikes = pd.DataFrame(USvideos['dislikes'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dislikes, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 4: Number of Trending Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "TD = pd.DataFrame(USvideos['trending days'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(TD, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 5: Total Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "CT = pd.DataFrame(USvideos['comment_total'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(CT, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 6: Average Sentiment Score\n",
    "- convert emojis (eg 😂) to text\n",
    "- convert emoticons (eg :D) to text\n",
    "- perform sentiment analysis to get the compound, positive, negative and neutral sentiment scores of each comment\n",
    "- sum up the compound sentiment scores of all the comments of each video\n",
    "- get the number of comments for each video\n",
    "- find the average sentiment score of each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing emot library\n",
    "!pip install emot\n",
    "#Importing libraries\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emojis into word\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \" \".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\"_\",\" \").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "text1 = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually fulfilling orders 😒\"\n",
    "convert_emojis(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UScomments['processed text'] = UScomments['comment_text'].apply(lambda x: convert_emojis(x))\n",
    "UScomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emoticons into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \" \".join(EMOTICONS[emot].replace(\",\",\"\").replace(\"_\",\" \").split()), text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "text = \"Hello :-) :-)\"\n",
    "convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UScomments['processed text'] = UScomments['processed text'].apply(lambda x: convert_emoticons(x))\n",
    "\n",
    "# look at row 74\n",
    "UScomments.head(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column of sentiment scores for visualisation\n",
    "UScomments['pos_score'] = UScomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['pos'])\n",
    "UScomments['neg_score'] = UScomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neg'])\n",
    "UScomments['neu_score'] = UScomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neu'])\n",
    "\n",
    "# we will use the compound score for further analysis. here the compound score is renamed as sentiment score\n",
    "UScomments['sentiment_scores'] = UScomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "UScomments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe for number of comments per video\n",
    "number_comments = UScomments['video_id'].value_counts().rename_axis('video_id').reset_index(name='number_comments')\n",
    "number_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new dataframe for total number of positive comments per video\n",
    "total = UScomments.groupby(['video_id'], sort = False).sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge total and number_comments dataframes \n",
    "total = pd.merge(total, number_comments, on = 'video_id')\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column of average sentiment\n",
    "total['average_sentiment'] = total['sentiment_scores'].div(total['number_comments'].values, axis=0)\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add average sentiment column into USvideos dataframe\n",
    "USvideos = pd.merge(USvideos, total, on = 'video_id')\n",
    "USvideos.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of the more commonly used words\n",
    "all_words = ' '.join([text for text in UScomments['comment_text']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "AS = pd.DataFrame(USvideos['average_sentiment'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(AS, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 7: Percentage of Positive Comments over Total Number of Comments\n",
    "###### for each comment: \n",
    "- if sentiment score > 0, it is a positive sentiment\n",
    "- if sentiment score < 0, it is a negative sentiment\n",
    "- if sentiment score > 0, it is a neutral sentiment\n",
    "\n",
    "- then find the percentage of comments with positive sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Categorize positive, negative, neutral\n",
    "UScomments['Sentiment'] = UScomments['sentiment_scores'].apply(lambda s : 'Positive' if s > 0 else ('Neutral' if s == 0 else 'Negative'))\n",
    "UScomments.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of comments which are positive in all the videos\n",
    "positive_percent = []\n",
    "for i in range(0,UScomments.video_id.nunique()):\n",
    "    a = UScomments[(UScomments.video_id == UScomments.video_id.unique()[i]) & (UScomments.Sentiment == 'Positive')].count()[0]\n",
    "    b = UScomments[UScomments.video_id == UScomments.video_id.unique()[i]]['Sentiment'].value_counts().sum()\n",
    "    Percentage = (a/b)*100\n",
    "    positive_percent.append(round(Percentage,2))\n",
    "\n",
    "positive_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for positive percentage\n",
    "positive_percentage = pd.DataFrame(positive_percent,UScomments.video_id.unique()).reset_index()\n",
    "positive_percentage.columns = ['video_id','Positive Percentage']\n",
    "positive_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add positive percentage column into USvideos dataframe\n",
    "USvideos = pd.merge(USvideos, positive_percentage, on = 'video_id')\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_posi = ' '.join([text for text in UScomments['comment_text'][UScomments.Sentiment == 'Positive']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of words in positive comments\n",
    "wordcloud_posi = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_posi)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud_posi, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "PP = pd.DataFrame(USvideos['Positive Percentage'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(PP, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 8: Percentage of Negative Comments over Total Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of comments which are negative in all the videos\n",
    "negative_percent = []\n",
    "for i in range(0,UScomments.video_id.nunique()):\n",
    "    a = UScomments[(UScomments.video_id == UScomments.video_id.unique()[i]) & (UScomments.Sentiment == 'Negative')].count()[0]\n",
    "    b = UScomments[UScomments.video_id == UScomments.video_id.unique()[i]]['Sentiment'].value_counts().sum()\n",
    "    Percentage = (a/b)*100\n",
    "    negative_percent.append(round(Percentage,2))\n",
    "\n",
    "negative_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for negative percentage\n",
    "negative_percentage = pd.DataFrame(negative_percent,UScomments.video_id.unique()).reset_index()\n",
    "negative_percentage.columns = ['video_id','Negative Percentage']\n",
    "negative_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add negative percentage column into USvideos dataframe\n",
    "USvideos = pd.merge(USvideos, negative_percentage, on = 'video_id')\n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_nega = ' '.join([text for text in UScomments['comment_text'][UScomments.Sentiment == 'Negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of words in negative comments\n",
    "wordcloud_nega = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_nega)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud_nega, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QP = pd.DataFrame(USvideos['quantile_popularity'])   # Response\n",
    "NP = pd.DataFrame(USvideos['Negative Percentage'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(NP, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(USvideos[\"quantile_popularity\"])\n",
    "X = pd.DataFrame(USvideos[[\"likes\", \"views\", \"comment_total\", \"trending days\", \"dislikes\", \"average_sentiment\", \"Positive Percentage\", \"Negative Percentage\"]])\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# Check the sample sizes\n",
    "print(\"Train Set :\", y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the distributions of all Predictors\n",
    "f, axes = plt.subplots(8, 3, figsize=(18, 16))\n",
    "\n",
    "count = 0\n",
    "for var in X_train:\n",
    "    sb.boxplot(X_train[var], orient = \"h\", ax = axes[count,0])\n",
    "    sb.distplot(X_train[var], ax = axes[count,1])\n",
    "    sb.violinplot(X_train[var], ax = axes[count,2])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Relationship between Response and the Predictors\n",
    "trainDF = pd.concat([y_train, X_train.reindex(index=y_train.index)], sort = False, axis = 1)\n",
    "\n",
    "f, axes = plt.subplots(8, 1, figsize=(18, 24))\n",
    "\n",
    "count = 0\n",
    "for var in X_train:\n",
    "    sb.boxplot(x = var, y = \"quantile_popularity\", data = trainDF, orient = \"h\", ax = axes[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Response corresponding to Predictors\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3c33d8227644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extract Response and Predictors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"quantile_popularity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"likes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"views\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"comment_total\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"trending days\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dislikes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"average_sentiment\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Positive Percentage\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Negative Percentage\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Split the Dataset into Train and Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(USvideos[\"quantile_popularity\"])\n",
    "X = pd.DataFrame(USvideos[[\"likes\", \"views\", \"comment_total\", \"trending days\", \"dislikes\", \"average_sentiment\", \"Positive Percentage\", \"Negative Percentage\"]])\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# Check the sample sizes\n",
    "print(\"Train Set :\", y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 100,  # n_estimators denote number of trees\n",
    "                                 max_depth = 7)       # set the maximum depth of each tree\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict 0/1 values corresponding to message\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Great Britain (GB)\n",
    "- Since the video statistics and comments statistics are in separate CSV files, we will import both files (GBvideos & GBcomments) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset and remove bad lines\n",
    "GBvideos = pd.read_csv('GBvideos.csv', error_bad_lines=False)\n",
    "GBvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset and remove bad lines\n",
    "GBcomments = pd.read_csv('GBcomments.csv', error_bad_lines=False)\n",
    "GBcomments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del thumbnail column as it is redundant for classification\n",
    "GBvideos = GBvideos.drop(columns='thumbnail_link')\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort videos from most views to least views for visualisation \n",
    "GBvideos.sort_values(by=['views'], inplace=True, ascending=False)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new DataFrame for number of duplicates to show the numberof duplicates\n",
    "duplicates_GB = GBvideos['video_id'].value_counts().rename_axis('video_id').reset_index(name='trending days')\n",
    "duplicates_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "GBvideos.drop_duplicates(subset =\"video_id\", keep = 'first', inplace = True)\n",
    "GBvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show that there are some videos with 0 views\n",
    "print(len(GBvideos[GBvideos['views'] == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the percentage of videos with 0 views is very small so its is okay to remove\n",
    "print((len(GBvideos[GBvideos['views']==0])) / len(GBvideos*100), '%', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with 0 views\n",
    "indexNames = GBvideos[GBvideos['views'] == 0].index\n",
    "GBvideos.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging 2 dataframes together, adding number of trending days\n",
    "GBvideos = pd.merge(GBvideos, duplicates_GB, on = 'video_id')\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at dataset again: 1736 videos change to 1734 videos after removal of 2 videos with 0 views\n",
    "GBvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it can be seen that there are some null objects under comment_text\n",
    "GBcomments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBcomments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at rows which comments are null\n",
    "print(GBcomments[GBcomments['comment_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with null comments\n",
    "len(GBcomments[GBcomments['comment_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the row index which comment_text are null\n",
    "indexNames_GB = GBcomments[GBcomments['comment_text'].isnull()].index\n",
    "indexNames_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows with null comments\n",
    "GBcomments.drop(indexNames_GB, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the 28 rows are dropped\n",
    "GBcomments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of rows in GBvideos whose video_id are NOT in GBcomments\n",
    "len(GBvideos[~GBvideos.video_id.isin(GBcomments.video_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the row index of rows in GBvideos whose video_id are NOT in GBcomments\n",
    "rowIndex3 = GBvideos[~GBvideos.video_id.isin(GBcomments.video_id)].index\n",
    "print(rowIndex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the rows in GBcomments whose video_id are NOT in GBvideos\n",
    "GBvideos.drop(rowIndex3, inplace=True)\n",
    "GBvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at dataset again after 42 videos were dropped\n",
    "GBvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at GBcomments dataset\n",
    "GBcomments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of rows in GBcomments whose video_id are NOT in GBvideos\n",
    "len(GBcomments[~GBcomments.video_id.isin(GBvideos.video_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set y as likes/views\n",
    "#sort based on likes over views \n",
    "GBvideos['popularity'] = GBvideos.apply(lambda row: 1000*(row.likes/row.views), axis = 1)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set y as (likes-dislikes)/views\n",
    "#sort based on (likes-dislikes)/views\n",
    "GBvideos['popularity2'] = GBvideos.apply(lambda row: 1000*(row.likes-row.dislikes)/row.views, axis = 1)\n",
    "\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of views \n",
    "GBvideos['views_rank'] = GBvideos['views'].rank(ascending = False)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of likes \n",
    "GBvideos['likes_rank'] = GBvideos['likes'].rank(ascending = False)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of popularity\n",
    "GBvideos['popularity_rank'] = GBvideos['popularity'].rank(ascending = False)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for ranking of popularity2\n",
    "GBvideos['popoularity2_rank'] = GBvideos['popularity2'].rank(ascending = False)\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GBvideos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1d0544cf0969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Spearmans correlation coefficient for views and likes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"likes_rank\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGBvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"views_rank\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Spearmans correlation coefficient: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GBvideos' is not defined"
     ]
    }
   ],
   "source": [
    "#Spearmans correlation coefficient for views and likes\n",
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(GBvideos[\"likes_rank\"], GBvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Spearman's correlation coefficient is not 1.000, which is different from the USvideos, this means that the videos with the highest views may not necessarily be the videos with the highest likes. Hence, we will have to compare the different features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearmans correlation coefficient for views and popularity\n",
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(GBvideos[\"popularity_rank\"], GBvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearmans correlation coefficient for views and popularity2\n",
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(GBvideos[\"popularity2_rank\"], GBvideos[\"views_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearmans correlation coefficient for popularity and likes\n",
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(GBvideos[\"likes_rank\"], GBvideos[\"popularity_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearmans correlation coefficient for popularity2 and likes\n",
    "from scipy.stats import spearmanr\n",
    "coef, p = spearmanr(GBvideos[\"likes_rank\"], GBvideos[\"popularity2_rank\"])\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the correlation coefficient for the ranking of likes is higher than that of views, it can be said that the popularity of the videos is more reliant on the number of likes rather than views. However, a similarity is that the correlation coefficient for popularity2 is lower for both likes and views, suggesting that the number is normalized. Hence, we will be using popularity2 as the response variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same response variable (ie. popularity2 = (likes-dislikes)/views) as derived earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_pop = go.Box(x = GBvideos['popularity2'], showlegend = False, name = \"popularity2\")\n",
    "py.iplot([GB_pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at overall dataset\n",
    "GBvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 points are very far away from the dataset, we will choose to separate them from the classification\n",
    "anomaly2 = GBvideos[GBvideos['popularity2']==GBvideos['popularity2'].min()]\n",
    "anomaly3 = GBvideos[GBvideos['popularity2']==GBvideos['popularity2'].max()]\n",
    "GBvideos = GBvideos.loc[GBvideos['popularity2']!=GBvideos['popularity2'].min()]\n",
    "GBvideos = GBvideos.loc[GBvideos['popularity2']!=GBvideos['popularity2'].max()]\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot for popularity2 after removing 2 anomaly\n",
    "GB_pop = go.Box(x = GBvideos['popularity2'], showlegend = False, name = \"popularity2\")\n",
    "py.iplot([GB_pop])\n",
    "\n",
    "#should we remove the maximum one also?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize the popularity2 of the videos into 4 categories according to their quartiles \n",
    "GBvideos['quantile_popularity'] = pd.cut(GBvideos['popularity2'], bins = 4, labels = ['low', 'very low', 'very high','high'])\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBvideos['quantile_popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBvideos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBvideos['popularity2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(GBvideos[[\"views\", \"likes\", \"dislikes\", \"comment_total\", \"trending days\"]]) \n",
    "# Plot the Raw Data on 2D grids\n",
    "sb.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the distribution of Response\n",
    "f, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "sb.boxplot(GBvideos['popularity2'], orient = \"h\", ax = axes[0], color = \"r\")\n",
    "sb.distplot(GBvideos['popularity2'], kde = False, ax = axes[1], color = \"r\")\n",
    "sb.violinplot(GBvideos['popularity2'], ax = axes[2], color = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot for popularity2\n",
    "trace = go.Histogram(x = GBvideos['popularity2'], histnorm = 'density')\n",
    "layout = go.Layout(title = 'Popularity2 Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# interactive plot for quantile_popularity\n",
    "trace = go.Histogram(x = GBvideos['quantile_popularity'], histnorm = 'density')\n",
    "layout = go.Layout(title = 'Quantile Popularity Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for Further Data Visualisation\n",
    "### Bi-variate clustering by Kmeans++ \n",
    "- views & popularity2 are used because views are seen as the most straightforward way of predicting popularity in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(GBvideos[[\"views\", \"popularity2\"]])\n",
    "\n",
    "# Set the Initialization to KMeans++\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Vary the Number of Clusters\n",
    "min_clust = 1\n",
    "max_clust = 40\n",
    "\n",
    "# Compute Within Cluster Sum of Squares\n",
    "within_ss = []\n",
    "for num_clust in range(min_clust, max_clust+1):\n",
    "    kmeans = KMeans(n_clusters = num_clust,        # number of clusters\n",
    "                    init = init_algo,              # initialization algorithm\n",
    "                    n_init = 5)                    # number of initializations\n",
    "    kmeans.fit(X)\n",
    "    within_ss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Within SS vs Number of Clusters\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,4))\n",
    "plt.plot(range(min_clust, max_clust+1), within_ss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within Cluster Sum of Squares')\n",
    "plt.xticks(np.arange(min_clust, max_clust+1, 1.0))\n",
    "plt.grid(which='major', axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"optimal\" Number of Clusters\n",
    "num_clust = 4\n",
    "\n",
    "# Set the Initialization to KMeans++\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Create Clustering Model using KMeans\n",
    "kmeans = KMeans(n_clusters = num_clust, init = init_algo, n_init = 20)                 \n",
    "\n",
    "# Fit the Clustering Model on the Data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Print the Cluster Centers\n",
    "print(\"Features\", \"\\tviews\", \"\\tpopularity2\")\n",
    "print()\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    print(\"Cluster\", i, end=\":\\t\")\n",
    "    for coord in center:\n",
    "        print(round(coord, 2), end=\"\\t\")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "# Print the Within Cluster Sum of Squares\n",
    "print(\"Within Cluster Sum of Squares :\", kmeans.inertia_)\n",
    "print()\n",
    "\n",
    "# Predict the Cluster Labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Append Labels to the Data\n",
    "X_labeled = X.copy()\n",
    "X_labeled[\"Cluster\"] = pd.Categorical(labels)\n",
    "\n",
    "# Summary of the Cluster Labels\n",
    "sb.countplot(X_labeled[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Clusters in the Data\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "plt.scatter(x = \"views\", y = \"popularity2\", c = \"Cluster\", cmap = 'viridis', data = X_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the Features against the Clusters\n",
    "f, axes = plt.subplots(2, 1, figsize=(16,8))\n",
    "sb.boxplot(x = 'views', y = 'Cluster', data = X_labeled, ax = axes[0])\n",
    "sb.boxplot(x = 'popularity2', y = 'Cluster', data = X_labeled, ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate Clustering by Kmeans++\n",
    "- we decided that bi-variate clustering was not enough, thus we decided to use this method to get a better overview of the dataset\n",
    "- categorical data (ie. Category ID) and non-numerical data (ie comment_text) are not used in clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(GBvideos[[\"likes\", \"views\", \"comment_total\", \"dislikes\"]]) \n",
    "\n",
    "\n",
    "# Plot the Raw Data on 2D grids\n",
    "sb.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the Number of Clusters\n",
    "min_clust = 1\n",
    "max_clust = 40\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Compute Within Cluster Sum of Squares\n",
    "within_ss = []\n",
    "for num_clust in range(min_clust, max_clust+1):\n",
    "    kmeans = KMeans(n_clusters = num_clust, init = init_algo, n_init = 5)\n",
    "    kmeans.fit(X)\n",
    "    within_ss.append(kmeans.inertia_)\n",
    "\n",
    "# Angle Plot : Within SS vs Number of Clusters\n",
    "f, axes = plt.subplots(1, 1, figsize=(16,4))\n",
    "plt.plot(range(min_clust, max_clust+1), within_ss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within Cluster Sum of Squares')\n",
    "plt.xticks(np.arange(min_clust, max_clust+1, 1.0))\n",
    "plt.grid(which='major', axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential models from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set \"optimal\" Clustering Parameters\n",
    "num_clust = 4\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "# Create Clustering Model using KMeans\n",
    "kmeans = KMeans(n_clusters = num_clust,         \n",
    "               init = init_algo,\n",
    "               n_init = 20)                 \n",
    "\n",
    "# Fit the Clustering Model on the Data\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Cluster Centers\n",
    "print(\"Features\", \"\\tlikes\", \"\\tviews\", \"\\tcomment_total\", \"\\tdislikes\",)\n",
    "print()\n",
    "\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    print(\"Cluster\", i, end=\":\\t\")\n",
    "    for coord in center:\n",
    "        print(round(coord, 2), end=\"\\t\")\n",
    "    print()\n",
    "print()\n",
    "\n",
    "# Print the Within Cluster Sum of Squares\n",
    "print(\"Within Cluster Sum of Squares :\", kmeans.inertia_)\n",
    "print()\n",
    "\n",
    "# Predict the Cluster Labels\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Append Labels to the Data\n",
    "X_labeled = X.copy()\n",
    "X_labeled[\"Cluster\"] = pd.Categorical(labels)\n",
    "\n",
    "# Summary of the Cluster Labels\n",
    "sb.countplot(X_labeled[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Clusters on 2D grids\n",
    "sb.pairplot(X_labeled, vars = X.columns.values, hue = \"Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for all Features against the Clusters\n",
    "f, axes = plt.subplots(4, 1, figsize=(16,24))\n",
    "sb.boxplot(x = 'likes', y = 'Cluster', data = X_labeled, ax = axes[0])\n",
    "sb.boxplot(x = 'views', y = 'Cluster', data = X_labeled, ax = axes[1])\n",
    "sb.boxplot(x = 'comment_total', y = 'Cluster', data = X_labeled, ax = axes[2])\n",
    "sb.boxplot(x = 'dislikes', y = 'Cluster', data = X_labeled, ax = axes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Average Behaviour of each Cluster\n",
    "cluster_data = pd.DataFrame(X_labeled.groupby(by = \"Cluster\").mean())\n",
    "cluster_data.plot.bar(figsize = (16,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1: Number of Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "likes = pd.DataFrame(GBvideos['likes'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(likes, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3: Number of Dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "dislikes = pd.DataFrame(GBvideos['dislikes'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dislikes, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 4: Number of Trending Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "TD = pd.DataFrame(GBvideos['trending days'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(TD, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 5: Total Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "CT = pd.DataFrame(GBvideos['comment_total'])       # Predictor\n",
    "\n",
    "# Split the Legendary-Total Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(CT, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Legendary values corresponding to Total\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 6: Average Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing emot library\n",
    "!pip install emot\n",
    "#Importing libraries\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emojis into word\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \" \".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\"_\",\" \").replace(\":\",\"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GBcomments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1bb1cbf7e4c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGBcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconvert_emojis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mGBcomments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GBcomments' is not defined"
     ]
    }
   ],
   "source": [
    "GBcomments['processed text'] = GBcomments['comment_text'].apply(lambda x: convert_emojis(x))\n",
    "GBcomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emoticons into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \" \".join(EMOTICONS[emot].replace(\",\",\"\").replace(\"_\",\" \").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBcomments['processed text'] = GBcomments['processed text'].apply(lambda x: convert_emoticons(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBcomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add column of sentiment scores for visualisation\n",
    "GBcomments['pos_score'] = GBcomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['pos'])\n",
    "GBcomments['neg_score'] = GBcomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neg'])\n",
    "GBcomments['neu_score'] = GBcomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neu'])\n",
    "\n",
    "# we will use the compound score for further analysis. here the compound score is renamed as sentiment score\n",
    "GBcomments['sentiment_scores'] = GBcomments['comment_text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "GBcomments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe for number of comments per video\n",
    "number_comments = GBcomments['video_id'].value_counts().rename_axis('video_id').reset_index(name='number_comments')\n",
    "number_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe for total number of positive comments per video\n",
    "total_GB = GBcomments.groupby(['video_id'], sort = False).sum()\n",
    "total_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge total and number_comments dataframes \n",
    "total_GB = pd.merge(total_GB, number_comments, on = 'video_id')\n",
    "total_GB = total_GB.drop(columns='likes')\n",
    "total_GB = total_GB.drop(columns='replies')\n",
    "total_GB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column of average sentiment\n",
    "total_GB['average_sentiment'] = total_GB['sentiment_scores'].div(total_GB['number_comments'].values, axis=0)\n",
    "total_GB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add average sentiment column into GBvideos dataframe\n",
    "GBvideos = pd.merge(GBvideos, total_GB, on = 'video_id')\n",
    "GBvideos.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in GBcomments['comment_text']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "AS = pd.DataFrame(GBvideos['average_sentiment'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(AS, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 7: Percentage of Positive Comments over Total Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorize positive, negative, neutral\n",
    "GBcomments['Sentiment'] = GBcomments['sentiment_scores'].apply(lambda s : 'Positive' if s > 0 else ('Neutral' if s == 0 else 'Negative'))\n",
    "GBcomments.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of comments which are positive in all the videos\n",
    "positive_percent = []\n",
    "for i in range(0,GBcomments.video_id.nunique()):\n",
    "    a = GBcomments[(GBcomments.video_id == GBcomments.video_id.unique()[i]) & (GBcomments.Sentiment == 'Positive')].count()[0]\n",
    "    b = GBcomments[GBcomments.video_id == GBcomments.video_id.unique()[i]]['Sentiment'].value_counts().sum()\n",
    "    Percentage = (a/b)*100\n",
    "    positive_percent.append(round(Percentage,2))\n",
    "\n",
    "positive_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for positive percentage\n",
    "positive_percentage = pd.DataFrame(positive_percent,GBcomments.video_id.unique()).reset_index()\n",
    "positive_percentage.columns = ['video_id','Positive Percentage']\n",
    "positive_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add positive percentage column into USvideos dataframe\n",
    "GBvideos = pd.merge(GBvideos, positive_percentage, on = 'video_id')\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_posi = ' '.join([text for text in GBcomments['comment_text'][GBcomments.Sentiment == 'Positive']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visulise words used in positive comments\n",
    "wordcloud_posi = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_posi)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud_posi, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0bffb8620e9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mQP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'quantile_popularity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mPP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBvideos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Positive Percentage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# Predictor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Split the Dataset into Train and Test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "PP = pd.DataFrame(GBvideos['Positive Percentage'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(PP, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 8: Percentage of Negative Comments over Total Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of comments which are negative in all the videos\n",
    "negative_percent = []\n",
    "for i in range(0,GBcomments.video_id.nunique()):\n",
    "    a = GBcomments[(GBcomments.video_id == GBcomments.video_id.unique()[i]) & (GBcomments.Sentiment == 'Negative')].count()[0]\n",
    "    b = GBcomments[GBcomments.video_id == GBcomments.video_id.unique()[i]]['Sentiment'].value_counts().sum()\n",
    "    Percentage = (a/b)*100\n",
    "    negative_percent.append(round(Percentage,2))\n",
    "\n",
    "negative_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for negative percentage\n",
    "negative_percentage = pd.DataFrame(negative_percent,GBcomments.video_id.unique()).reset_index()\n",
    "negative_percentage.columns = ['video_id','Negative Percentage']\n",
    "negative_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add negative percentage column into USvideos dataframe\n",
    "GBvideos = pd.merge(GBvideos, negative_percentage, on = 'video_id')\n",
    "GBvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_nega = ' '.join([text for text in GBcomments['comment_text'][GBcomments.Sentiment == 'Negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise words used in negative comments\n",
    "wordcloud_nega = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_nega)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud_nega, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QP = pd.DataFrame(GBvideos['quantile_popularity'])   # Response\n",
    "NP = pd.DataFrame(GBvideos['Negative Percentage'])       # Predictor\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(NP, QP, test_size = 0.25)\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 7)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict response values corresponding to predictor\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(GBvideos[\"quantile_popularity\"])\n",
    "X = pd.DataFrame(GBvideos[[\"likes\", \"views\", \"comment_total\", \"dislikes\", \"average_sentiment\", \"trending days\", \"Positive Percentage\", \"Negative Percentage\"]])\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# Check the sample sizes\n",
    "print(\"Train Set :\", y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw the distributions of all Predictors\n",
    "f, axes = plt.subplots(8, 3, figsize=(18, 16))\n",
    "\n",
    "count = 0\n",
    "for var in X_train:\n",
    "    sb.boxplot(X_train[var], orient = \"h\", ax = axes[count,0])\n",
    "    sb.distplot(X_train[var], ax = axes[count,1])\n",
    "    sb.violinplot(X_train[var], ax = axes[count,2])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Relationship between Response and the Predictors\n",
    "trainDF = pd.concat([y_train, X_train.reindex(index=y_train.index)], sort = False, axis = 1)\n",
    "\n",
    "f, axes = plt.subplots(8, 1, figsize=(18, 24))\n",
    "\n",
    "count = 0\n",
    "for var in X_train:\n",
    "    sb.boxplot(x = var, y = \"quantile_popularity\", data = trainDF, orient = \"h\", ax = axes[count])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 10)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "# Predict Response corresponding to Predictors\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "# Plot the Decision Tree\n",
    "treedot = export_graphviz(dectree,                                      # the model\n",
    "                          feature_names = X_train.columns,              # the features \n",
    "                          out_file = None,                              # output file\n",
    "                          filled = True,                                # node colors\n",
    "                          rounded = True,                               # make pretty\n",
    "                          special_characters = True)                    # postscript\n",
    "\n",
    "graphviz.Source(treedot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Response and Predictors\n",
    "y = pd.DataFrame(GBvideos[\"quantile_popularity\"])\n",
    "X = pd.DataFrame(GBvideos[[\"likes\", \"views\", \"comment_total\", \"dislikes\", \"average_sentiment\",\"trending days\", \"Positive Percentage\", \"Negative Percentage\"]])\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# Check the sample sizes\n",
    "print(\"Train Set :\", y_train.shape, X_train.shape)\n",
    "print(\"Test Set  :\", y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestClassifier model from Scikit-Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the Random Forest object\n",
    "rforest = RandomForestClassifier(n_estimators = 100,  # n_estimators denote number of trees\n",
    "                                 max_depth = 10)  # set the maximum depth of each tree\n",
    "\n",
    "# Fit Random Forest on Train Data\n",
    "rforest.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import confusion_matrix from Scikit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict 0/1 values corresponding to message\n",
    "y_train_pred = rforest.predict(X_train)\n",
    "y_test_pred = rforest.predict(X_test)\n",
    "\n",
    "# Check the Goodness of Fit (on Train Data)\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", rforest.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", rforest.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Comparing US and GB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique USvideos:\", USvideos['video_id'].count())\n",
    "print(\"Number of unique GBvideos:\", GBvideos['video_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = USvideos['popularity2']\n",
    "x1 = GBvideos['popularity2']\n",
    "\n",
    "fig = go.Figure()\n",
    "# Use x instead of y argument for horizontal plot\n",
    "fig.add_trace(go.Box(x=x0, name = \"USvideos\"))\n",
    "fig.add_trace(go.Box(x=x1, name = \"GBvideos\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = USvideos['popularity2']\n",
    "x1 = GBvideos['popularity2']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choice of Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) Definition\n",
    "\n",
    "We defined popular as **(likes-dislikes)/views** (termed as **popularity2** in our dataset).\n",
    "\n",
    "This was subjective as we based it on our instincts as to what might be the best response to use for this dataset. However, we felt that this was the most appropriate response after carrying out Spearman Correlation Ranking as we have normalised it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) Spearman’s rank correlation vs Pearson’s rank correlation? \n",
    "The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables. In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate. The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data.\n",
    "Since we are trying to find the best response variable for our project, we want to use a response variable that is based on the relationships of the variables rather than the raw data since the raw data between the most views and next most views could fluctuate a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By sorting the dataset according to views (highest at the top), we found out that there are duplicted videos. Since the dataset consists of videos that have been trending for 30 days, duplicated videos happen because some videos have been trending for more than 1 day. Hence, we took into account of this and removed all the duplicated videos, leaving only the latest one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Categorical Data for Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also recognised that single, multivariate decision trees require the response (ie popularity2) to be discrete/categorised. However, popularity2 is a continuous variable.\n",
    "\n",
    "Hence, we brought in **quantile_popularity** which splits popularity2 into 4 catogories by their quartile. Each quartile has the same range, but might not have the same number of videos. The categories are: \"very low\", \"low\", \"high\", \"very high\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one limitation of quantile_popularity is that there is an **uneven distribution** of videos in the 4 categories for quantile_popularity. As can be seen below, the category \"high\", has only 16 videos. \n",
    "\n",
    "This meant that the trained data for this category might not be very reliable and accurate. Results might be more biased towards videos with \"very low\" quantile_popularity as there is a larger dataset for it to be trained and tested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# interactive plot for quantile_popularity\n",
    "trace = go.Histogram(x = USvideos['quantile_popularity'], histnorm = 'density')\n",
    "layout = go.Layout(title = 'Quantile Popularity Distribution')\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly\n",
    "#### 5a) Separate Analysis for anomaly1 in USvideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anomaly1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value of popularity2 and the difference between likes and dislikes, we can infer that this video is not well received. Let's take a look at the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis on Anomaly1\n",
    "\n",
    "anomaly1_comments = UScomments.loc[UScomments['video_id']== anomaly1.iloc[0]['video_id']]\n",
    "anomaly1_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to convert emoji into words\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \" \".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\"_\",\" \").replace(\":\",\"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anomaly1_comments['processed text'] = anomaly1_comments['comment_text'].apply(lambda x: convert_emojis(x))\n",
    "anomaly1_comments.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emoticons into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \" \".join(EMOTICONS[emot].replace(\",\",\"\").replace(\"_\",\" \").split()), text)\n",
    "    return text\n",
    "anomaly1_comments['processed text'] = anomaly1_comments['processed text'].apply(lambda x: convert_emoticons(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column of sentiment scores for visualisation\n",
    "anomaly1_comments['pos_score'] = anomaly1_comments['comment_text'].apply(lambda x:sia.polarity_scores(x)['pos'])\n",
    "anomaly1_comments['neg_score'] = anomaly1_comments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neg'])\n",
    "anomaly1_comments['neu_score'] = anomaly1_comments['comment_text'].apply(lambda x:sia.polarity_scores(x)['neu'])\n",
    "\n",
    "# we will use the compound score for further analysis. here the compound score is renamed as sentiment score\n",
    "anomaly1_comments['sentiment_scores'] = anomaly1_comments['comment_text'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
    "anomaly1_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe for number of comments per video\n",
    "number_comments1 = anomaly1_comments['video_id'].value_counts().rename_axis('video_id').reset_index(name='number_comments1')\n",
    "number_comments1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe for total number of positive comments per video\n",
    "anomaly1_total = anomaly1_comments.groupby(['video_id'], sort = False).sum()\n",
    "anomaly1_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge anomaly1_total and number_comments dataframes \n",
    "anomaly1_total = pd.merge(anomaly1_total, number_comments1, on = 'video_id')\n",
    "anomaly1_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column of average sentiment\n",
    "anomaly1_total['average_sentiment'] = anomaly1_total['sentiment_scores'].div(anomaly1_total['number_comments1'].values, axis=0)\n",
    "anomaly1_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# words used in this anomaly1 video\n",
    "all_words = ' '.join([text for text in anomaly1_comments['comment_text']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the statistics of this video in detail, it can be seen that this video is about the politics in US. It can be a prime example of a video that is not well received. However, we chose to exclude this video in our classification as we considered this video as an anomaly. The popularity2 value is pretty extreme and might skew our data. That being said, if this project was about finding out which features were important in classifying videos that are not well received, this video might be important in the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b) Univariate Anomaly Detection\n",
    "\n",
    "We decided to ultimately remove 1 anomaly via box-plot visualisation as explained above. However, observation might not be the best justification for anomalies. Thus, to be more sure that the point we removed is indeed an anomaly, one improvement that could have been done was to use univarite anomaly detection method shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(USvideos.shape[0]), np.sort(USvideos['popularity2'].values))\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('popularity2')\n",
    "plt.title(\"Popularity2 Distribution\")\n",
    "sb.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.distplot(USvideos['popularity2'])\n",
    "plt.title(\"Distribution of Popularity2\")\n",
    "sb.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using IsolationForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "isolation_forest = IsolationForest(n_estimators=100)\n",
    "isolation_forest.fit(USvideos['popularity2'].values.reshape(-1, 1))\n",
    "xx = np.linspace(USvideos['popularity2'].min(), USvideos['popularity2'].max(), len(USvideos)).reshape(-1,1)\n",
    "anomaly_score = isolation_forest.decision_function(xx)\n",
    "outlier = isolation_forest.predict(xx)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(xx, anomaly_score, label='anomaly score')\n",
    "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n",
    "                 where=outlier==-1, color='r', \n",
    "                 alpha=.4, label='outlier region')\n",
    "plt.legend()\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel('popularity2')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5c) Multivariate Anomaly Detection\n",
    "\n",
    "Another method is to set up a Multi-Variate Anomaly Detection problem on the USvideos Dataset.   \n",
    "Features : **views, likes, dislikes, comment_total, sentiment_scores, trending days, positive percentage, negative percentage**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features from the Data\n",
    "X = pd.DataFrame(USvideos[[\"views\", \"likes\", \"dislikes\", \"comment_total\", \"trending days\", \"average_sentiment\", \"Positive Percentage\", \"Negative Percentage\"]]) \n",
    "\n",
    "# Plot the Raw Data on 2D grids\n",
    "sb.pairplot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LocalOutlierFactor from sklearn.neighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Set the Parameters for Neighborhood\n",
    "num_neighbors = 20      # Number of Neighbors\n",
    "cont_fraction = 0.05    # Fraction of Anomalies\n",
    "\n",
    "# Create Anomaly Detection Model using LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors = num_neighbors, contamination = cont_fraction)\n",
    "\n",
    "# Fit the Model on the Data and Predict Anomalies\n",
    "lof.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Anomalies\n",
    "labels = lof.fit_predict(X)\n",
    "\n",
    "# Append Labels to the Data\n",
    "X_labeled = X.copy()\n",
    "X_labeled[\"Anomaly\"] = pd.Categorical(labels)\n",
    "\n",
    "# Summary of the Anomaly Labels\n",
    "sb.countplot(X_labeled[\"Anomaly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the Anomalies in the Data\n",
    "sb.pairplot(X_labeled, vars = X.columns.values, hue = \"Anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same could be applied to GBvideos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Number of Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might seem that high views would mean high popularity. However, as can be seen, the univariate classification accuracy for views is only around 63%. This shows that views alone is not a good indicator of popularity. A few other factors come together to determine popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = USvideos['views']\n",
    "x1 = GBvideos['views']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy for both USvideos and GBvideos are similar, at 0.64. This shows that views as a features performs equally in both datasets.\n",
    "\n",
    "However, 'views' as a feature alone is not strong enough as the classification accuracy of 0.64 is pretty low. This reinforces high views does not mean high popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Number of Likes and Dislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = USvideos['likes']\n",
    "x1 = GBvideos['likes']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking note that there are fewer GBvideos and that the number of likes GBvideos get are moderately higher, this might play a part in the importance of the feature 'likes'. As derived above, the classification accuracy for the feature 'likes' for USvideos is around 0.752 and that of GBvideos is 0.676. \n",
    "\n",
    "This might be because the number of likes is moderately high for a fewer number of videos and hence 'likes' as a feature is not as significant for GBvideos. Hence, the lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = USvideos['dislikes']\n",
    "x1 = GBvideos['dislikes']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, 'dislikes' as a feature, on the other hand, is pretty similar across both datasets. Classification accuracy for USvideos is 0.646 and that if GBvideos is 0.630."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Trending Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = USvideos['trending days']\n",
    "x1 = GBvideos['trending days']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some videos that were trending for a few days. To improve on the classification, we could have performed a more indepth analysis of why these videos were trending for so long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, 'trending days' as a feature is not a strong feature as well. Classification accuracy is at 0.552 for USvideos and 0.619 for GBvideos. This might be because the youtube trending algorithm is different and based on different aspects of the video, hence the number of trending days is reliant on the popularity of the video, and not the other way round. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x0 = USvideos['comment_total']\n",
    "x1 = GBvideos['comment_total']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy for USvideos is 0.684 and that for GBvideos is 0.635. Many factors might contribute to the difference in accuracies. One might be the higher number of data in USvideos and this allows the dataset to be better trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Average Sentiment Score\n",
    "##### 10a) Translation\n",
    "TextBlob analyses other language comments but it is a paid service. It only analyses up to 500 words per day. Hence, we could not implement text translation into our project. However, if possible, the codes below can be added in for translation, so that our dataset can be more robust in terms of the sentimental analysis feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at how textblob translate languages\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"Ä°lk baÅŸta gÃ¶sterilen yerde tÃ¼rkÃ§e olarak bÃ¼yÃ¼k karakterlerle kongre merkezi yazÄ±lmÄ±ÅŸ olmasÄ± da ilginÃ§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.translate(to= 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that can translate non-english comments to english\n",
    "\n",
    "def translate_comment(text):\n",
    "    blob = TextBlob(text)\n",
    "    if blob.detect_language() == 'en':\n",
    "        return text\n",
    "    else: \n",
    "        return blob.translate(to= 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try out the function\n",
    "\n",
    "translate_comment(\"Ä°lk baÅŸta gÃ¶sterilen yerde tÃ¼rkÃ§e olarak bÃ¼yÃ¼k karakterlerle kongre merkezi yazÄ±lmÄ±ÅŸ olmasÄ± da ilginÃ§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #apply to the whole dataset\n",
    "\n",
    "# UScomments[‘processed text’] = US[‘processed text’].apply(lambda x: translate_comment(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as mentioned earlier, this code cannot be run on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if sentiment analyser works on foreign languages\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "print(sentiment_analyzer_scores('Ä°lk baÅŸta gÃ¶sterilen yerde tÃ¼rkÃ§e olarak bÃ¼yÃ¼k karakterlerle kongre merkezi yazÄ±lmÄ±ÅŸ olmasÄ± da ilginÃ§.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check with the english translation\n",
    "print(sentiment_analyzer_scores(\"It is also interesting that the congress center was written with great characters in the place shown in the first place.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is shown that setimental analysis do not work with foreign languages and hence those comments with foreign languages will have a sentiment score of 0. Thus, we have included 2 features, namely Percentage of Positive Comments and Percentage of Negative Comments to help us make our model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10b) Spelling Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to load spell check using text blob. However, dataset is too huge and alot of time will be taken. Hence, we could not carry out spelling check on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spelling check by first removing letters that are repeated >2 times consecutively\n",
    "# english words have a max of 2 letters that are repeeated one after another\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "#example\n",
    "sentence = \"she is amazzzinggg\"\n",
    "updated = reduce_lengthening(sentence)\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UScomments['processed text'] = UScomments['processed text'].apply(lambda x: reduce_lengthening(str(x)))\n",
    "UScomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TextBlob to do spelling correction\n",
    "from textblob import TextBlob\n",
    "\n",
    "#example\n",
    "print(str(TextBlob(updated).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carry out spelling correction for first 5 rows\n",
    "#can be seen that it is not very accurate\n",
    "#slangs are not corrected correctly\n",
    "UScomments['comment_text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #do for whole dataset\n",
    "\n",
    "# UScomments['spell checked text'] = UScomments['comment_text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "# UScomments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, that being said, after conducting spelling check for the first 5 rows for exploration, it can be seen that spelling check by TextBlob is not very accurate. Hence, spell check might not be of high significance to the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10c) Reason for not cleaning text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We handpicked Vader Sentiment because it is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. We realised that Vader is a relatively sensitive tool that takes into account several characteristics of a text, therefore we decided not to proceed on with the text data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i) Uppercase letters\n",
    "Using upper case letters to emphasize a sentiment-relevant word in the presence of other non-capitalized words, increases the magnitude of the sentiment intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Changing the text to lower case\n",
    "# UScomments['comment_text'] = UScomments['comment_text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii) Conjunctions\n",
    "Use of conjunctions like “but” signals a shift in sentiment polarity, with the sentiment of the text following the conjunction being dominant. “The food here is great, but the service is horrible” has mixed sentiment, with the latter half dictating the overall rating. (Decided not to lemmatize or remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Lemmatization\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# tokenized_UScomments.apply(lambda x: [wnl.lemmatize(i) for i in x if i not in set(stopwords.words('english'))]) \n",
    "# tokenized_UScomments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii) Punctuation, Special Characters, Numbers\n",
    "\n",
    "We found out that these symbols are usually part of emoticons that is an essential feature of determining the sentiment of the comment. Emoticons are used extensively in social media texts (in this case 'youtube comments')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing Punctuations, Numbers and Special Characters\n",
    "\n",
    "# UScomments['comment_text'] = UScomments['comment_text'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other challenges faced conducting sentiment analysis\n",
    "- Context and polarity\n",
    "- Irony and sarcasm\n",
    "- Comparison\n",
    "- Defining what is neutral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10d) Use of Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found out that tags do not contribute much to the classification, as they have a very low correlation, as seen from the very low R-Squared value. Hence we are excluding tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical imports\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from pprint import pprint\n",
    "#Returns a sorted histogram dataframe (with top_n rows) for a given list.\n",
    "def form_hist(given_list,top_n):\n",
    "\n",
    "    item_set = set(given_list)\n",
    "    items = []\n",
    "    counts = []\n",
    "    for nm in item_set:\n",
    "        items.append(nm)\n",
    "        counts.append(given_list.count(nm))\n",
    "    return pd.DataFrame({'count':counts,'items':items}).sort_values(by='count',ascending=False).head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top20_UStags(videos, num, title):\n",
    "\n",
    "    all_tags = videos['tags'].map(lambda k: k.lower().split('|')).values\n",
    "    all_tags = [item for sublist in all_tags for item in sublist]\n",
    "\n",
    "    counts = form_hist(all_tags,num)\n",
    "    counts.columns = ['count','tags']\n",
    "    plt.figure()\n",
    "    sb.barplot(x = counts['tags'], y = counts['count'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('count')\n",
    "    plt.title(title)\n",
    "\n",
    "top20_UStags(USvideos,20,'Top 20 US Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tags_as_feature(videos, k):\n",
    "    #Determine the top k tags\n",
    "    videos = videos.copy()\n",
    "    all_tags = videos['tags'].map(lambda k: k.lower().split('|'))\n",
    "    all_tags = [item for sublist in all_tags for item in sublist]\n",
    "    counts = form_hist(all_tags,k)\n",
    "    top_tags = counts['items'].values[:k]\n",
    "\n",
    "    def filter_f(x):\n",
    "        x = x.lower().split('|')\n",
    "        return [e for e in x if e in top_tags]\n",
    "\n",
    "    #Reduce tags to only the most frequent ones\n",
    "    videos['tags'] = videos['tags'].map(filter_f)\n",
    "\n",
    "    #Convert our data into the design matrix\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    design = mlb.fit_transform(videos['tags'])\n",
    "    design = sm.add_constant(design)\n",
    "\n",
    "\n",
    "    #Fit linear regression\n",
    "    ols = sm.OLS(videos['views'].values, design)\n",
    "    fitting = ols.fit()\n",
    "    labels = ['intercept'] + list(mlb.classes_)\n",
    "    return fitting.summary(), labels\n",
    "\n",
    "top_n_tags = 20\n",
    "\n",
    "USsummary, uslabels = tags_as_feature(USvideos, top_n_tags)\n",
    "print(\"US VIDEOS\")\n",
    "pprint(USsummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10e) Vader (Rule-Based) vs other Machine Learning-Automatic Approach like Naive Bayes, SVM etc\n",
    "Advantage of Vader: \n",
    "- Doesn’t require any training data but is constructed from a generalizable, valence-based, human-curated gold standard sentiment lexicon\n",
    "- works exceedingly well on social media type text, yet readily generalizes to multiple domains\n",
    "- fast enough to be used online with streaming data\n",
    "\n",
    "Disadvantage of ML:\n",
    "- depend on the training set to represent as many features as possible (which often, they do not – especially in the case of the short, sparse text of social media)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Positive Percentage and Negative Perentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = USvideos['Positive Percentage']\n",
    "x1 = GBvideos['Positive Percentage']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy is around 0.64 for USvideos and 0.72 for GBvideos. It is also noted that the classification accuracy for percentage of positive comments vary a lot between US and GB (65% vs 72%), but similar for percentage of negative comments (both 70%). This might be because words of negative sentiment might be more unique hence they might perform better with sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = USvideos['Negative Percentage']\n",
    "x1 = GBvideos['Negative Percentage']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=x0 , name = \"USvideos\"))\n",
    "fig.add_trace(go.Histogram(x=x1, name = \"GBvideos\"))\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy is similar at around 0.70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Category ID\n",
    "#### 12a) Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the categories of videos that are present in dataset\n",
    "USvideos.category_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(USvideos.category_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USvideos.category_id.value_counts().plot(kind='pie', autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the old category id list\n",
    "print(\"Category ID List used for this dataset:\")\n",
    "OCI = pd.read_csv('old_category_id.csv')\n",
    "OCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above category list, it can be seen that Category ID 24 which corresponds to **Entertainment** has the highest count. This might seem expected as people usually watch entertainment videos on YouTube, hence contributing to the high value count. \n",
    "\n",
    "However, on further inspection, it is also good to take note that Entertainment is actually a general category for all the other sub-categories like comedy, shows, music. Hence, video creators might have just chosen Entertainment as it is a broader cateogory. \n",
    "\n",
    "Thus, deriving Entertainment as the category of highest count might not be very useful in classification. It might be better if we are getting the genres (eg horror, comedy, sports) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent years, YouTube has also developed a new Cateogry ID List, hence this classification for Category ID might not be applicable for current videos. The new list can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the updated category id list\n",
    "print(\"Updated Category ID List used currently by YouTube (which this dataset does not use):\")\n",
    "UCI = pd.read_csv('updated_category_id.csv')\n",
    "UCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to use **one hot encoding** for the classification of Category IDs to ensure that the data is normalised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12b) One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "onehotencoder = OneHotEncoder()\n",
    "#reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "X = onehotencoder.fit_transform(USvideos.category_id.values.reshape(-1,1)).toarray()\n",
    "#To add this back into the original dataframe \n",
    "dfOneHot = pd.DataFrame(X, columns = [\"category_id_\"+str(int(i)) for i in range(0,16)]) \n",
    "USvideos = pd.concat([USvideos, dfOneHot], axis=1)\n",
    "#printing to verify \n",
    "USvideos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we realised that although one hot encoding is a decent tool to normalise categorised variables, categorised variables itself is not a good feature of Decision Tree classifications. By one-hot encoding a categorical variable, we are inducing sparsity into the dataset and that is undesirable.\n",
    "\n",
    "During the splitting algorithm, the answer will be a yes or no (ie binary). Thus, the tree will only grow in one direction. One hot encoding categorical variables with high cardinality can cause inefficiency in tree-based classifications. \n",
    "\n",
    "Continuous variables will be given more importance than the categorical variables by the algorithm. Categorical variables will obscure the order of feature importance resulting in poorer performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Choice of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding on the most suitable ML Model for our project, we considered Logistic Regression as well as Support Vector Machine (SVM). However, both are intended for binary (two-class) classification problems, which does not fit our 4 quantile model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it is to be acknowledged that the dataset itself has **limitations**. Some of the features are somehow correlated with each other. Higher trending days count might be due to having high number of views and likes etc. Hence, we attempted to normalise the dataset by choosing the most appropriate response indicator (ie (likes-dislikes)/views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other improvements that could have been done for this project could have been to include the analysis of likes and replies for each comment. If this project could be brought further, it could help in the recommendation of YouTube videos to users in different countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of Features for **USvideos**:\n",
    "    1. Number of Likes\n",
    "    2. Percentage of Negative Comments over Total Number of Comments\n",
    "    3. Average Sentiment\n",
    "    4. Total Number of Comments\n",
    "    5. Percentage of Positive Comments over Total Number of Comments\n",
    "    6. Number of Dislikes\n",
    "    7. Number of Views\n",
    "    8. Trending Days\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking of Features for **GBvideos**:\n",
    "    1. Percentage of Positive Comments over Total Number of Comments\n",
    "    2. Average Sentiment\n",
    "    3. Percentage of Negative Comments over Total Number of Comments\n",
    "    4. Number of Likes\n",
    "    5. Number of views\n",
    "    6. Total Number of Comments\n",
    "    7. Number of Dislikes\n",
    "    8. Trending Days\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking above shows that the features might be of different importance in different countries. This might be due to the cultural factors. For example, the way people type the comments - the tone, emojis and slangs might play a part in sentimental analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If possible, a different recommendation system can be used for US and GB as the types of videos that Americans prefer watching might be different from those that British prefer to watch, or the range of videos or genres that the people watch are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-variate Decision Tree is better than the single decision tree as it combines all the features, hence all features can be used at different levels of the tree to give a high classification accuracy.\n",
    "\n",
    "Random forest is able to generalize much better to the testing data than the single decision tree or the multi-variate decision tree. The random forest has lower variance while maintaining the low bias of a decision tree. This is because the random forest is essentially a collection of decision trees.\n",
    "\n",
    "A decision tree is built on an entire dataset, using all the features as mentioned earlier whereas a random forest randomly selects specific features to build multiple decision trees from and then averages the results.\n",
    "\n",
    "Thus, we can conclude that the random forest will give the highest classification accuracy and that the 8 features listed above will serve to provide better predications in classifications. From the rankings above, it can be seen that average sentiment is a pretty good feature in classsifcation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "- https://www.kaggle.com/datasnaek/youtube#GBcomments.csv\n",
    "- https://www.kaggle.com/datasnaek/youtube-new#GBvideos.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://techpostplus.com/2019/04/26/youtube-video-categories-list-faqs-and-solutions/\n",
    "- https://gist.github.com/dgp/1b24bf2961521bd75d6c\n",
    "- https://www.kaggle.com/minc33/k-means-clustering-vs-logistic-regression\n",
    "- https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
    "- https://towardsdatascience.com/k-means-clustering-8e1e64c1561c\n",
    "- https://towardsdatascience.com/unsupervised-learning-clustering-algorithms-5b290967f746\n",
    "- https://data-flair.training/blogs/k-means-clustering-tutorial/\n",
    "- https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/\n",
    "- https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\n",
    "- https://medium.com/towards-artificial-intelligence/emoticon-and-emoji-in-text-mining-7392c49f596a\n",
    "- http://datameetsmedia.com/staging/3908/vader-sentiment-analysis-explained/\n",
    "- https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "- https://textblob.readthedocs.io/en/dev/quickstart.html#translation-and-language-detection\n",
    "- https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
